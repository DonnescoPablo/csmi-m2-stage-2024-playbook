<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Google Cortex Framework :: My Intership Report</title>
    <link rel="canonical" href="https://docs.demo.com/report/v1.0/tasks-and-contributions/cortex.html">
    <meta name="generator" content="Antora 3.1.7">
    <link rel="stylesheet" href="../../../_/css/site.css">
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://docs.demo.com">My Intership Report</a>
      <button class="navbar-burger" aria-controls="topbar-nav" aria-expanded="false" aria-label="Toggle main menu">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <!-- Existing menu items can go here if needed -->
        <div class="navbar-item">
          <img src="../../../_/img/Capgemini_Logo_White_Mono_RGB.svg" alt="Company Logo" style="height: 80px; margin-right: 5px;">
          <img src="../../../_/img/UniversitÃ©_de_Strasbourg.svg.png" alt="University Logo" style="height: 50px;">
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="report" data-version="v1.0">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <button class="nav-menu-toggle" aria-label="Toggle expand/collapse all" style="display: none"></button>
    <h3 class="title"><a href="../index.html">Report</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="description.html">Tasks and Contributions</a>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="cortex.html">Google Cortex Framework</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Report</span>
    <span class="version">v1.0</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <div class="title"><a href="../index.html">Report</a></div>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">v1.0</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Report</a></li>
    <li><a href="description.html">Tasks and Contributions</a></li>
    <li><a href="cortex.html">Google Cortex Framework</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Google Cortex Framework</h1>
<div class="sect1">
<h2 id="_what_is_it"><a class="anchor" href="#_what_is_it"></a>What is it ?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Google Cortex Framework is a comprehensive and flexible analytics solution
designed to help businesses leverage the full potential of their data.
It integrates data processing pipelines, machine learning capabilities,
and visualization tools to enable organizations to make data-driven decisions
efficiently and effectively.
Using its packaged content, you can reduce the risk, complexity and cost of getting started.</p>
</div>
<div class="paragraph">
<p>Key components of the Google Cortex Framework include :</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Data Ingestion</dt>
<dd>
<p>It supports the seamless integration of data from <strong>various sources</strong>.</p>
</dd>
<dt class="hdlist1">Data Processing</dt>
<dd>
<p>The framework employs advanced data processing techniques, such as <strong>batch</strong> and
<strong>stream processing</strong>, to handle large volumes of data in <strong>real-time</strong>. This
allows businesses to gain insights from their data almost instantaneously.</p>
</dd>
<dt class="hdlist1">Machine Learning</dt>
<dd>
<p>The framework leverages Google&#8217;s cutting-edge machine learning models and tools
to perform predictive analytics, anomaly detection, and other complex analyses.
This helps organizations to uncover hidden patterns and trends in their data.</p>
</dd>
<dt class="hdlist1">Data Visualization</dt>
<dd>
<p>The framework include a robust visualization tool that enable users to use
already built-in interactive and intuitive dashboards or create custom ones.
This helps in understanding the data better and making informed
decisions based on the insights gained.</p>
</dd>
<dt class="hdlist1">Scalability</dt>
<dd>
<p>Built on Google&#8217;s cloud infrastructure, the Google Cortex Framework is highly
scalable, allowing businesses to process and analyze massive datasets without
compromising performance.</p>
</dd>
<dt class="hdlist1">Security</dt>
</dl>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_architecture"><a class="anchor" href="#_the_architecture"></a>The architecture</h2>
<div class="sectionbody">
<div class="paragraph">
<div class="title">Google Cortex Framework Architecture</div>
<p><span class="image"><img src="_images/cortex_framework.png" alt="Google Cortex Framework Architecture"></span></p>
</div>
<div class="paragraph">
<p>The framework is designed to provide a comprehensive data and application
integration architecture leveraging the power of Google Cloud services. Bellow
is a detailed breakdown of the components and their interactions within the stack,
starting from the foundational layer and moving upwards.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Data Sources</dt>
<dd>
<p>The architecture supports data ingestion from a variety of sources, including <strong>SAP</strong>,
<strong>Salesforce</strong>, <strong>Google Ads</strong>, and more.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Since new data sources are added regularly, you can find a complete list of supported
data sources <a href="https://github.com/GoogleCloudPlatform/cortex-data-foundation?tab=readme-ov-file#operational-related-data-sources">here</a>.</p>
</div>
</td>
</tr>
</table>
</div>
</dd>
<dt class="hdlist1">Data Ingestion and Integration Tools</dt>
<dd>
<p>To ingest data from sources, you have the choice between using native tools
provided by Google and other tools provided by third-party partners.</p>
<div class="paragraph">
<p>If you choose to use Google&#8217;s native tools, depending on the data source, there
are predifined data ingestion templates that you can use to quickly ingest data.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>You can check out the Data integration guides for SAP on Google Cloud <a href="https://cloud.google.com/solutions/sap/docs/sap-data-integration-guides">here</a>.</p>
</div>
</td>
</tr>
</table>
</div>
</dd>
</dl>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Cortex Data Foundation</dt>
<dd>
<p>The Cortex Data Foundation is the core layer of the framework. It is anchored
on <strong>BigQuery</strong>, the Google Cloud petabyte-scale <strong>Datawarehouse</strong> as the core architectural component.</p>
<div class="paragraph">
<p>The Data Foundation provides packaged solution content for strategic business
data sources like SAP, Salesforce and others and includes :</p>
</div>
<div class="ulist">
<ul>
<li>
<p>predefined templates to help with ingestion and <strong>change data capture</strong> pipelines</p>
</li>
<li>
<p>predefined data models to support rapid insights</p>
</li>
<li>
<p>simple machine learning templates and dashboards for different business use case scenarios.</p>
</li>
<li>
<p>deployment accelerators to help with the deployment of the solution content.</p>
<div class="paragraph">
<p>More technically,</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Raw Landing Datasets</dt>
<dd>
<p>Raw data from sources is ingested and stored in BigQuery. This forms the basis
for further processing and analysis.</p>
</dd>
<dt class="hdlist1">Change Data Capture Processed Datasets</dt>
<dd>
<p>This layer processes changes in the raw landing datasets, ensuring that the most up-to-date information is available.</p>
</dd>
<dt class="hdlist1">Cross Workload Reporting Dataset</dt>
<dd>
<p>Using the Data Capture Processed Datasets, you can create specific reporting
datasets for specific sources. You can also cross them with other partner
datasets to create new reporting datasets to best gain insights from the data.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>As long as data is landed in BigQuery in its raw format with the same granularity
as it is in the source, the predifined Cortex templates will work.</p>
</div>
</td>
</tr>
</table>
</div>
</dd>
</dl>
</div>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">Application &amp; Integration, Visualization</dt>
<dd>
<p>You can finally consume your processed data through a variety of tools, including :</p>
<div class="dlist">
<dl>
<dt class="hdlist1">Looker</dt>
<dd>
<p>Looker is a powerful data visualization tool that allows you to create
interactive dashboards and reports to visualize your data.
Even better, there are predefined Looker dashboards and reports for different business scenarios.</p>
</dd>
<dt class="hdlist1">Vertex AI</dt>
<dd>
<p>Vertex AI is a machine learning platform that allows you to build, deploy,
and manage machine learning models. You can use it for instance to predict
demand based on your processed data to improve your decision-making process.</p>
</dd>
</dl>
</div>
</dd>
</dl>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Google Cloud Cortex Data Foundation is open source. You can use all or parts of
the available content, and you can adjust it to add any other data sources
or customizations you may need for ultimate flexibility.</p>
</div>
<div class="paragraph">
<p>You can find the source files on GitHub by clicking <a href="https://github.com/GoogleCloudPlatform/cortex-data-foundation">here</a>.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_dive_into_my_work"><a class="anchor" href="#_dive_into_my_work"></a>Dive into my work</h2>
<div class="sectionbody">
<div class="paragraph">
<p>With the Google Cortex Framework, my specific role involved :</p>
</div>
<div class="ulist">
<ul>
<li>
<p>studying how the framework is architecturaly set up</p>
</li>
<li>
<p>deploying <strong>SAP workload</strong> into BigQuery</p>
</li>
<li>
<p>configuring CI/CD workflow for automating the deployment</p>
</li>
<li>
<p>using looker to visualize the data</p>
</li>
<li>
<p>using powerBI to visualize the data.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Let&#8217;s dive into the details of each of these tasks.</p>
</div>
<div class="sect2">
<h3 id="_deploying_sap_workload_into_bigquery"><a class="anchor" href="#_deploying_sap_workload_into_bigquery"></a>Deploying SAP workload into BigQuery</h3>
<div class="paragraph">
<p>In this section we are going to use the Google Cortex Framework&#8217;s predefined
templates and deployment accelerators to deploy a SAP workload into BigQuery.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>We don&#8217;t have access to a SAP system for this demo, so we are going to use SAP generated data.
That implies that we are note going to use any integration tools to ingest the data into BigQuery.
It will be automatically generated and stored in BigQuery.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Before we start, make sure you have a Google Cloud Platform account with some credits to use the services.
You can subscribe to a free trial <a href="https://cloud.google.com/pricing">here</a>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The free trial gives you $300 in credits to use on Google Cloud services for 90 days.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="_establish_project_and_dataset_structure"><a class="anchor" href="#_establish_project_and_dataset_structure"></a>Establish project and dataset structure</h4>
<div class="paragraph">
<p>You need to create at least one project to host the BigQuery datasets and execute
the deployment process. For more details click <a href="https://github.com/GoogleCloudPlatform/cortex-data-foundation?tab=readme-ov-file#establish-project-and-dataset-structure">here</a>.</p>
</div>
<div class="paragraph">
<p>In this demo, we are going to create a project called <code>cortex-demo</code>. Access the
Google Cloud Console and create a new project.</p>
</div>
<div class="paragraph">
<p>Click on the current project name at the top of the page.</p>
</div>
<div class="paragraph">
<p>Click on the <code>New Project</code> button.</p>
</div>
<div class="paragraph">
<p>Fill in the project name and click on the <code>Create</code> button. If you&#8217;re a member of
an organization, you can select the organization to link the project to.</p>
</div>
<div class="paragraph">
<p>Once the project is created, you can see it in the project list. Click on it to
access the project dashboard.</p>
</div>
<div class="paragraph">
<p>Now that we have a project, we need to create datasets to store the data. We are
going to create two datasets, one for the raw data and one for the processed data.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>With the framework&#8217;s default configuration, it is mandatory to create the two datasets
mentioned above.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>To create the datasets, follow the steps below :</p>
</div>
<div class="paragraph">
<p>Search for <code>BigQuery</code> in the search bar at the top of the page.</p>
</div>
<div class="paragraph">
<p>Among the results, in the section <code>PRODUCTS &amp; PAGES</code> click on <code>BigQuery (Data warehouse/analytics)</code>.</p>
</div>
<div class="paragraph">
<p>In the panel on the left, click on the three horizontal lines to open the menu located next to
the project ID.</p>
</div>
<div class="paragraph">
<p>Choose the action <code>Create dataset</code>.</p>
</div>
<div class="paragraph">
<p>Fill in the dataset name, choose the location and click on the <code>Create dataset</code> button.
In this tutorial, let&#8217;s name the first dataset <code>SAP_RAW_LANDING</code> and the second one <code>SAP_CDC_PROCESSED</code>.</p>
</div>
<div class="paragraph">
<p>Repeat the same steps to create the second dataset.</p>
</div>
<div class="paragraph">
<p>Once the datasets are created, you can see them by unravelling the project in the panel on the left.</p>
</div>
</div>
<div class="sect3">
<h4 id="_configure_google_cloud_platform_components"><a class="anchor" href="#_configure_google_cloud_platform_components"></a>Configure Google Cloud Platform components</h4>
<div class="paragraph">
<p>Before deploying the SAP workload, we need to enable some Google Cloud Platform components :</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">BigQuery</dt>
<dd>
<p>We need to enable the BigQuery API to allow the framework to interact with BigQuery.</p>
</dd>
<dt class="hdlist1">Cloud Build</dt>
<dd>
<p>Cloud Build is a service that executes your builds on Google Cloud Platform infrastructure.
We need to enable it to allow the framework use it for the deployment process.</p>
</dd>
<dt class="hdlist1">Cloud Composer</dt>
<dd>
<p>Cloud Composer is a fully managed workflow orchestration service. In our case, it ensures
that data processing tasks are executed in the correct order at the right time.</p>
</dd>
<dt class="hdlist1">Cloud Storage</dt>
<dd>
<p>Cloud Storage is a storage component of the Google Cloud Platform. In our case, it will
be used to store the deployment artifacts.</p>
</dd>
<dt class="hdlist1">Cloud Resource Manager</dt>
<dd>
<p>Cloud Resource Manager is a service that manages Google Cloud resources. We need to enable it
to allow the framework to create and manage resources.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>To enable these components, access to the cloud shell by clicking on the <code>Activate Cloud Shell</code>
as shown in the image below.</p>
</div>
<div class="paragraph">
<p>Then, run the following commands to enable the components :</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">gcloud config set project &lt;SOURCE_PROJECT&gt; <i class="conum" data-value="1"></i><b>(1)</b>

gcloud services enable bigquery.googleapis.com \
                       cloudbuild.googleapis.com \
                       composer.googleapis.com \
                       storage-component.googleapis.com \
                       cloudresourcemanager.googleapis.com</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Replace <code>&lt;SOURCE_PROJECT&gt;</code> with the project ID you created earlier.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>You should get a success message :</p>
</div>
</div>
<div class="sect3">
<h4 id="_grant_permissions_to_the_cloud_build_service_account"><a class="anchor" href="#_grant_permissions_to_the_cloud_build_service_account"></a>Grant permissions to the Cloud Build service account</h4>
<div class="paragraph">
<p>Cloud Build needs to use a special service account to execute the deployment process.
To do so, we need to grant the necessary permissions to that account.</p>
</div>
<div class="paragraph">
<p>By default, a Cloud Build service account is automatically tied to the project.
You can locate it in <code>IAM</code>.</p>
</div>
<div class="paragraph">
<p>In the Google Cloud Console, search for <code>IAM</code> in the search bar at the top of the page,
and click on <code>IAM &amp; Admin</code> in the results.</p>
</div>
<div class="paragraph">
<p>Then, In the <code>IAM</code> page, click on <code>GRANT ACCESS</code> as shown in the image below.</p>
</div>
<div class="paragraph">
<p>In the opened window, fill in the <code>New principals</code> field with the term <code>cloudbuild</code>,
and select the principal that appears in the dropdown list with the following format :
<code>[PROJECT_NUMBER]@cloudbuild.gserviceaccount.com - Legacy Cloud Build Service Account</code>.</p>
</div>
<div class="paragraph">
<p>Then grant the following roles to the service account :</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">BigQuery Data Editor</dt>
<dd>
<p>This role allows the service account to edit all the contents of datasets in BigQuery.</p>
</dd>
<dt class="hdlist1">BigQuery Job User</dt>
<dd>
<p>This role allows the service account to run jobs in BigQuery.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>Save the changes by clicking on the <code>ADD</code> button.</p>
</div>
</div>
<div class="sect3">
<h4 id="_create_a_storage_bucket_for_storing_dag_related_files"><a class="anchor" href="#_create_a_storage_bucket_for_storing_dag_related_files"></a>Create a Storage bucket for storing DAG related files</h4>
<div class="paragraph">
<p>A storage bucket is required to store processing DAG scripts and other temporary files
generated during the deployment.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The generated scripts will have to be manually moved into a Cloud Composer instance
after deployment.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Navigate to <code>Cloud Storage</code> in the Google Cloud Console.</p>
</div>
<div class="paragraph">
<p>Then, create a new bucket by clicking on the <code>CREATE</code> button. Let&#8217;s name it <code>cortex-demo-dags-bucket</code>.</p>
</div>
<div class="paragraph">
<p>Then click on the <code>CONTINUE</code> button and select a location for the bucket. Let&#8217;s choose the region
where the BigQuery datasets are located (<code>eu (multiple regions in European Union)</code>).</p>
</div>
<div class="paragraph">
<p>Then click on <code>CONTINUE</code> and leave the default settings for the rest of the configuration by clicking
on the <code>CREATE</code> button at the end.</p>
</div>
<div class="paragraph">
<p>Once the bucket is created, navigate to the <code>PERMISSIONS</code> tab to grant the <code>Storage Object Creator</code>
role to the Cloud Build service account.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The <code>Storage Object Creator</code> role allows the Cloud Build service account to just create objects in the bucket.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_create_a_storage_bucket_for_logs"><a class="anchor" href="#_create_a_storage_bucket_for_logs"></a>Create a Storage bucket for logs</h4>
<div class="paragraph">
<p>We are going to create another bucket to store the logs generated during the deployment process.
Follow the same steps as above to create a new bucket. Let&#8217;s name it <code>cortex-demo-logs-bucket</code>.
Once the bucket is created, navigate to the <code>PERMISSIONS</code> tab to grant the <code>Storage Object Admin</code>
permission to the Cloud Build service account.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The <code>Storage Object Admin</code> role allows the Cloud Build service account to create, delete, and manage objects in the bucket.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_configure_the_deployment"><a class="anchor" href="#_configure_the_deployment"></a>Configure the deployment</h4>
<div class="sect4">
<h5 id="_clone_the_cortex_data_foundation_repository"><a class="anchor" href="#_clone_the_cortex_data_foundation_repository"></a>Clone the Cortex Data Foundation repository</h5>
<div class="paragraph">
<p>Now that we have all the necessary components enabled and permissions granted, we can configure the deployment.
First, we have to clone the Cortex Data Foundation repository to get the deployment scripts and configuration files.
In the cloud shell, run the following command :</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">git clone --recurse-submodules https://github.com/GoogleCloudPlatform/cortex-data-foundation</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then, navigate to the <code>cortex-data-foundation</code> directory :</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">cd cortex-data-foundation</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="_global_deployment_configuration"><a class="anchor" href="#_global_deployment_configuration"></a>Global deployment configuration</h5>
<div class="paragraph">
<p>The behavior of the deployment is controlled by the configuration file <a href="https://github.com/GoogleCloudPlatform/cortex-data-foundation/blob/main/config/config.json">config.json</a>.</p>
</div>
<div class="paragraph">
<p>Open the file in the Editor by clicking on <code>Open Editor</code> on the Cloud Shell Editor page as shown in the image below.</p>
</div>
<div class="paragraph">
<p>Then open the Folder <code>cortex-data-foundation</code> in the editor.</p>
</div>
<div class="paragraph">
<p>Then fill the content of the file <code>config/config.json</code> with the following content :</p>
</div>
<div id="config-json-id" class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">{
    "testData": true, <i class="conum" data-value="1"></i><b>(1)</b>
    "deploySAP": true, <i class="conum" data-value="2"></i><b>(2)</b>
    "deploySFDC": false, <i class="conum" data-value="3"></i><b>(3)</b>
    "deployMarketing": false, <i class="conum" data-value="4"></i><b>(4)</b>
    "deployDataMesh": false, <i class="conum" data-value="5"></i><b>(5)</b>
    "turboMode": true, <i class="conum" data-value="6"></i><b>(6)</b>
    "projectIdSource": "cortex-demo-423814", <i class="conum" data-value="7"></i><b>(7)</b>
    "projectIdTarget": "cortex-demo-423814", <i class="conum" data-value="8"></i><b>(8)</b>
    "targetBucket": "cortex-demo-dags-bucket", <i class="conum" data-value="9"></i><b>(9)</b>
    "location": "EU", <i class="conum" data-value="10"></i><b>(10)</b>
    "languages": [ "E" ], <i class="conum" data-value="11"></i><b>(11)</b>
    "currencies": [ "USD" ], <i class="conum" data-value="12"></i><b>(12)</b>
    "testDataProject": "kittycorn-public", <i class="conum" data-value="13"></i><b>(13)</b>
    "k9": {
        "datasets": {
            "processing": "K9_PROCESSING", <i class="conum" data-value="14"></i><b>(14)</b>
            "reporting": "K9_REPORTING" <i class="conum" data-value="15"></i><b>(15)</b>
        }
    },
    "SAP": {
        "deployCDC": true, <i class="conum" data-value="16"></i><b>(16)</b>
        "datasets": {
            "cdc": "SAP_CDC_PROCESSED", <i class="conum" data-value="17"></i><b>(17)</b>
            "raw": "SAP_RAW_LANDING", <i class="conum" data-value="18"></i><b>(18)</b>
            "ml": "SAP_ML_MODELS", <i class="conum" data-value="19"></i><b>(19)</b>
            "reporting": "SAP_REPORTING" <i class="conum" data-value="20"></i><b>(20)</b>
        },
        "SQLFlavor": "ecc", <i class="conum" data-value="21"></i><b>(21)</b>
        "mandt": "100" <i class="conum" data-value="22"></i><b>(22)</b>
    },
    "shareWithCredly": false, <i class="conum" data-value="23"></i><b>(23)</b>
    "userInfo": { <i class="conum" data-value="24"></i><b>(24)</b>
        "email": "",
        "firstName": "",
        "lastName": ""
    }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Set to <code>true</code> to generate test data. Since we don&#8217;t have access to a SAP system, we are going to use test data.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Set to <code>true</code> to execute the deployment for SAP workload.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Set to <code>false</code> to skip the deployment for Salesforce workload.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Set to <code>false</code> to skip the deployment for Marketing workload.</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Set to <code>false</code> to skip the deployment for Data Mesh.</td>
</tr>
<tr>
<td><i class="conum" data-value="6"></i><b>6</b></td>
<td>Set to <code>true</code> to speed up the deployment process.</td>
</tr>
<tr>
<td><i class="conum" data-value="7"></i><b>7</b></td>
<td>The project ID where the source dataset is and the build will run.</td>
</tr>
<tr>
<td><i class="conum" data-value="8"></i><b>8</b></td>
<td>The project ID for user-facing datasets (reporting and ML datasets).</td>
</tr>
<tr>
<td><i class="conum" data-value="9"></i><b>9</b></td>
<td>The name of the bucket where the DAGs will be stored.</td>
</tr>
<tr>
<td><i class="conum" data-value="10"></i><b>10</b></td>
<td>Location where the BigQuery dataset and GCS buckets are</td>
</tr>
<tr>
<td><i class="conum" data-value="11"></i><b>11</b></td>
<td>Languages to be used in analytics models. <code>E</code> stands for English.</td>
</tr>
<tr>
<td><i class="conum" data-value="12"></i><b>12</b></td>
<td>Currencies to be used in analytics models. <code>USD</code> stands for US Dollar.</td>
</tr>
<tr>
<td><i class="conum" data-value="13"></i><b>13</b></td>
<td>The source of the test data for demo deployments.</td>
</tr>
<tr>
<td><i class="conum" data-value="14"></i><b>14</b></td>
<td>Execute cross-workload templates (e.g., date dimension) as defined in the <a href="https://github.com/GoogleCloudPlatform/cortex-data-foundation/blob/main/src/k9/config/k9_settings.yaml">K9 configuration file</a>.</td>
</tr>
<tr>
<td><i class="conum" data-value="15"></i><b>15</b></td>
<td>Execute cross-workload templates and external data sources (e.g. Weather) as defined in the <a href="https://github.com/GoogleCloudPlatform/cortex-data-foundation/blob/main/src/k9/config/k9_settings.yaml">K9 configuration file</a>. It is commented out by default.</td>
</tr>
<tr>
<td><i class="conum" data-value="16"></i><b>16</b></td>
<td>Set to <code>true</code> to deploy the Change Data Capture (CDC) process.</td>
</tr>
<tr>
<td><i class="conum" data-value="17"></i><b>17</b></td>
<td>The name of the dataset where the processed data will be stored.</td>
</tr>
<tr>
<td><i class="conum" data-value="18"></i><b>18</b></td>
<td>The name of the dataset where the raw data will be stored.</td>
</tr>
<tr>
<td><i class="conum" data-value="19"></i><b>19</b></td>
<td>The name of the dataset where the machine learning models will be stored.</td>
</tr>
<tr>
<td><i class="conum" data-value="20"></i><b>20</b></td>
<td>The name of the dataset where the reporting data will be stored.</td>
</tr>
<tr>
<td><i class="conum" data-value="21"></i><b>21</b></td>
<td>The SQL flavor to use for the SAP deployment.</td>
</tr>
<tr>
<td><i class="conum" data-value="22"></i><b>22</b></td>
<td>The client number to use for the SAP deployment.</td>
</tr>
<tr>
<td><i class="conum" data-value="23"></i><b>23</b></td>
<td>Set to <code>true</code> to share the deployment badge with Credly.</td>
</tr>
<tr>
<td><i class="conum" data-value="24"></i><b>24</b></td>
<td>User information for the badge to be granted.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>For more information about the configuration file, refer to <a href="https://github.com/GoogleCloudPlatform/cortex-data-foundation?tab=readme-ov-file#global-deployment-configuration">this section</a> of the official documentation.</p>
</div>
<div class="paragraph">
<p>Once the configuration file is filled, save the changes.</p>
</div>
</div>
<div class="sect4">
<h5 id="_customizing_reporting_settings_file_configuration"><a class="anchor" href="#_customizing_reporting_settings_file_configuration"></a>Customizing <code>reporting_settings</code> file configuration</h5>
<div class="paragraph">
<p>All the configuration files and templates related to SAP reporting models are
located in the <code>src/SAP/SAP_REPORTING</code> directory. Besides other files, this
directory contains 03 settings files that drive how the BigQuery objects
(tables or views) for SAP Reporting datasets are created. These files are :</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>reporting_settings_ecc.yaml</code></p>
</li>
<li>
<p><code>reporting_settings_s4.yaml</code></p>
</li>
<li>
<p><code>reporting_settings_union.yaml</code></p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>During the deployment process, only one of these files will be used, depending on the
SQL flavor defined in the global configuration file. In our case, because we set the
SQL flavor to <code>ecc</code>, the <code>reporting_settings_ecc.yaml</code> file will be used. That behavior
is defined in the deployment script <code>src/SAP/SAP_REPORTING/cloudbuild.reporting.yaml</code>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Whether the BigQuery objects are tables or views you can specify other informations.
For instance, if it is a table, you can specify how often the table will be refreshed
by the Cloud Composer task orchestrator. For more information about the configuration,
refer to this <a href="https://github.com/GoogleCloudPlatform/cortex-data-foundation?tab=readme-ov-file#customizing-reporting_settings-file-configuration">section</a> of the official documentation.</p>
</div>
<div class="paragraph">
<p>In our case, we are going to use the default settings.</p>
</div>
</div>
<div class="sect4">
<h5 id="_execute_the_deployment"><a class="anchor" href="#_execute_the_deployment"></a>Execute the deployment</h5>
<div class="paragraph">
<p>Now that the configuration is set, we can execute the deployment process.</p>
</div>
<div class="paragraph">
<p>In the cloud shell, run the following command :</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">gcloud builds submit --project &lt;SOURCE_PROJECT&gt; \ <i class="conum" data-value="1"></i><b>(1)</b>
    --substitutions=_GCS_BUCKET=&lt;Bucket for logs&gt; <i class="conum" data-value="2"></i><b>(2)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Replace <code>&lt;SOURCE_PROJECT&gt;</code> with the project ID you created earlier.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Replace <code>&lt;Bucket for logs&gt;</code> with the name of the bucket where the logs will be stored. In our case, it is <code>cortex-demo-logs-bucket</code>.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>This command will trigger the deployment process, using the build configuration
file <code>cloudbuild.yaml</code> at the root. It will take some time to complete. You can
track the progress in the Cloud Build page in the Google Cloud Console as shown in the images below.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Ensure that you are in the <code>cortex-data-foundation</code> directory before running the command.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_configure_cloud_composer"><a class="anchor" href="#_configure_cloud_composer"></a>Configure Cloud Composer</h4>
<div class="paragraph">
<p>Once the deployment process is complete, you need to configure Cloud Composer to
run the processing DAGs. The DAGs are generated during the deployment process and
stored in the bucket <code>cortex-demo-dags-bucket</code>.</p>
</div>
<div class="sect4">
<h5 id="_create_a_cloud_composer_environment"><a class="anchor" href="#_create_a_cloud_composer_environment"></a>Create a Cloud Composer environment</h5>
<div class="paragraph">
<p>Navigate to <code>Cloud Composer</code> in the Google Cloud Console.</p>
</div>
<div class="paragraph">
<p>We are going to create a cloud composer environment. To do so, Click on the
<code>CREATE ENVIRONMENT</code>, then <code>Composer 1</code>.</p>
</div>
<div class="paragraph">
<p>Fill in the environment name, and select the location as shown in the image below.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>You can name the environment differently. You can also choose a different location.
However, depending on the location, it may be impossible to create the composer 1 environment.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Then grant the required permissions to the Cloud Composer service account as shown in the image below.</p>
</div>
<div class="paragraph">
<p>Leave the default settings for the rest of the configuration and click on the <code>CREATE</code> button.</p>
</div>
<div class="paragraph">
<p>Once the environment is created, navigate to the <code>PYPI PACKAGES</code> tab and click on <code>EDIT</code>.</p>
</div>
<div class="paragraph">
<p>Then and add the following  :</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>pytrends~=4.9.2</code></p>
</li>
<li>
<p><code>holidays</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Then click on the <code>SAVE</code> button to save the changes.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Those packages are required for the processing DAGs to run successfully.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Since we leave the default settings for the rest of the configuration, a bucket is automatically
created for the environment. That&#8217;s where Cloud Composer will look for the DAGs.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect4">
<h5 id="_set_up_a_connection_to_bigquery"><a class="anchor" href="#_set_up_a_connection_to_bigquery"></a>Set up a connection to BigQuery</h5>
<div class="paragraph">
<p>Before Cloud Composer can run the processing DAGs, we need to set up a connection to BigQuery in Airflow.</p>
</div>
<div class="paragraph">
<p>First, let&#8217;s create a service account to use for the connection. Go to the <code>IAM &amp; Admin</code>
page and click on <code>Service Accounts</code> as shown in the image below.</p>
</div>
<div class="paragraph">
<p>Then click on the <code>CREATE SERVICE ACCOUNT</code> button.</p>
</div>
<div class="paragraph">
<p>Fill in the service account name and description, and click on the <code>CREATE AND CONTINUE</code> button.</p>
</div>
<div class="paragraph">
<p>Then grant the following role <code>Editor</code> to the service account click on the <code>CONTINUE</code> button.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p>It is always recommended to have least privileges on resources.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Then click on the <code>DONE</code> button.</p>
</div>
<div class="paragraph">
<p>Once the service account is created, click on it to access the details.</p>
</div>
<div class="paragraph">
<p>Then go to the <code>KEYS</code> tab and click on the <code>ADD KEY</code> button and choose <code>Create new key</code>.</p>
</div>
<div class="paragraph">
<p>Choose the key type as <code>JSON</code> and click on the <code>CREATE</code> button.</p>
</div>
<div class="admonitionblock caution">
<table>
<tr>
<td class="icon">
<i class="fa icon-caution" title="Caution"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The private key will be downloaded to your local machine. It will allow access
to cloud resources, so keep it secure.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Now the next step is to configure the connection in Airflow UI.
From the Cloud Composer main page, click on <code>Airflow</code> as shown in the image below.</p>
</div>
<div class="paragraph">
<p>Then on the Airflow UI, click on <code>Admin</code> and then <code>Connections</code>.</p>
</div>
<div class="paragraph">
<p>Click on the plus sign to add a new connection.</p>
</div>
<div class="paragraph">
<p>Fill in the connection details as shown in the image below.</p>
</div>
<div class="paragraph">
<p>Then click on the <code>TEST</code> button to test the connection.</p>
</div>
<div class="paragraph">
<p>Once the connection is successful, click on the <code>SAVE</code> button to save the connection.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Note that I name the connection <code>sap_cdc_bq</code>. That is the connection name used in the processing DAGs.</p>
</div>
<div class="paragraph">
<p>Also, the connection type is <code>Google Cloud</code>. The <code>Keyfile JSON</code> field is the JSON key file content that is downloaded earlier.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Then click on the <code>SAVE</code> button to save the connection.</p>
</div>
<div class="paragraph">
<p>By following the steps above, create a connection for the Reporting DAGs as well. The connection name should be <code>sap_reporting_bq</code>.</p>
</div>
<div class="paragraph">
<p>For more information about the connection name, refer to this <a href="https://github.com/GoogleCloudPlatform/cortex-data-foundation?tab=readme-ov-file#gathering-cloud-composer-settings">section</a>
of the official documentation.</p>
</div>
</div>
<div class="sect4">
<h5 id="_copy_the_dags_to_the_cloud_composer_bucket"><a class="anchor" href="#_copy_the_dags_to_the_cloud_composer_bucket"></a>Copy the DAGs to the Cloud Composer bucket</h5>
<div class="paragraph">
<p>The last step is to copy the processing DAGs generated during the deployment process to the Cloud Composer bucket.</p>
</div>
<div class="paragraph">
<p>Navigate to the cloud shell and run the following command :</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">gsutil -m cp -r  gs://&lt;output bucket&gt;/dags/ gs://&lt;composer dag bucket&gt;/
gsutil -m cp -r  gs://&lt;output bucket&gt;/data/ gs://&lt;composer sql bucket&gt;/</code></pre>
</div>
</div>
<div class="paragraph">
<p>Replace <code>&lt;output bucket&gt;</code> with the bucket where the DAGs are stored (<code>cortex-demo-dags-bucket</code>).
Replace <code>&lt;composer dag bucket&gt;</code> with the bucket where the Cloud Composer DAGs are stored.
In our case, <code>&lt;composer sql bucket&gt;</code> is the same as <code>&lt;composer dag bucket&gt;</code>.</p>
</div>
<div class="paragraph">
<p>Once the DAGs are copied, you can check the Airflow UI to see that the tasks are in execution.</p>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_configuring_cicd_workflow_for_automating_the_deployment"><a class="anchor" href="#_configuring_cicd_workflow_for_automating_the_deployment"></a>Configuring CI/CD workflow for automating the deployment</h3>
<div class="paragraph">
<p>Suppose we have the source code of the Cortex Data Foundation on Github and we
want to change the refresh frequency of the reporting tables. To ensure that
the new parameters are immediately applied, a best practice would be to
automate the deployment of the new frequencies once they are defined.</p>
</div>
<div class="paragraph">
<p>To achieve this, we will use tags in our version control process. The specific
steps are as follows :</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Link the Github repository to the Google Cloud project</strong></p>
<div class="paragraph">
<p>Establish a connection between our Github repository and our Google Cloud project
to ensure smooth and continuous integration.</p>
</div>
</li>
<li>
<p><strong>Configure a trigger in Google Cloud</strong></p>
<div class="paragraph">
<p>Set up a trigger that will monitor the creation of new tags on Github. Each time
a tag is created, this trigger will automatically initiate the build process.</p>
</div>
</li>
</ol>
</div>
<div class="sect3">
<h4 id="_link_the_github_repository_to_the_google_cloud_project"><a class="anchor" href="#_link_the_github_repository_to_the_google_cloud_project"></a>Link the Github repository to the Google Cloud project</h4>
<div class="paragraph">
<p>First, we need to set up a Github repository to store the source code of the
Cortex Data Foundation. We are going to fork the Data Foundation repository
and its submodules repositories . So let&#8217;s start by forking the following repository :</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/GoogleCloudPlatform/cortex-data-foundation/tree/main">Cortex Data Foundation</a></p>
</li>
<li>
<p><a href="https://github.com/GoogleCloudPlatform/cortex-dag-generator/tree/fbf65bfd7331578127db5c115e8aa22489db6eea">cortex-dag-generator</a></p>
</li>
<li>
<p><a href="https://github.com/GoogleCloudPlatform/cortex-reporting/tree/c78e40d55d13365999a77238601a3989bb52d7e2">cortex-reporting</a></p>
</li>
<li>
<p><a href="https://github.com/GoogleCloudPlatform/cortex-ml-models/tree/46406c5f58baf466151010bb7049e57342f6db7a">cortex-ml-models</a></p>
</li>
<li>
<p><a href="https://github.com/GoogleCloudPlatform/cortex-salesforce/tree/3644129d92eca0dbc30b3a5f9c8065a4f72eb3f2">cortex-salesforce</a></p>
</li>
<li>
<p><a href="https://github.com/GoogleCloudPlatform/cortex-marketing/tree/29fbee1dbf3101e16f97c39fdb0b8539cb87de3f">cortex-marketing</a></p>
</li>
</ul>
</div>
<div class="admonitionblock caution">
<table>
<tr>
<td class="icon">
<i class="fa icon-caution" title="Caution"></i>
</td>
<td class="content">
<div class="paragraph">
<p>A github account is required to fork the repository. If you don&#8217;t have one, you
can create one.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Once the repositories are forked, we are going to clone them to our local machine.
Before that, we are going to generate an SSH key to authenticate with Github.
Follow this <a href="https://docs.github.com/fr/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account">tutorial</a> to do so. Then, clone the forked
Cortex Data Foundation repository by running the following
command in the cloud shell :</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">git clone --recurse-submodules &lt;SSH URL of the forked Cortex Data Foundation repository&gt; cortex-data-foundation-ci-cd</code></pre>
</div>
</div>
<div class="admonitionblock caution">
<table>
<tr>
<td class="icon">
<i class="fa icon-caution" title="Caution"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Replace <code>&lt;SSH URL of the forked Cortex Data Foundation repository&gt;</code> with the SSH URL of the forked repository.
<code>cortex-data-foundation-ci-cd</code> is the name of the directory where the repository will be cloned. We
provide this additional name to avoid conflicts with the existing directory (created previously).</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Navigate to the component <code>Repositories</code> in the Google Cloud Console.</p>
</div>
<div class="paragraph">
<p>The in the <code>2ND GEN</code> tab, click on <code>CREATE HOST CONNECTION</code>.</p>
</div>
<div class="paragraph">
<p>On the left panel select <code>GitHub</code> as your source provider and select a region and a fill
a name for your connection.</p>
</div>
<div class="paragraph">
<p>Then click on the <code>CONNECT</code> button.</p>
</div>
<div class="paragraph">
<p>You will get <code>Secret Manager API is not enabled</code> message. Click on <code>VIEW SECRET MANAGER API</code>.</p>
</div>
<div class="paragraph">
<p>Then click on the <code>ENABLE</code> button on the <code>Secret Manager API</code> page.</p>
</div>
<div class="paragraph">
<p>Once the enabled, go back to the previous page and click on the <code>CONNECT</code> button.</p>
</div>
<div class="paragraph">
<p>A new window will open. Click on the <code>Continue</code> button.</p>
</div>
<div class="paragraph">
<p>Then select the GitHub account you want to use.</p>
</div>
<div class="paragraph">
<p>Select <code>Only select repositories</code>, select all repositories related to the Cortex Data Foundation and click on <code>Install</code>.</p>
</div>
<div class="paragraph">
<p>We have successfully created a connection to the Github repository. Now link the Data Foundation repository
to the connection as shown below.</p>
</div>
</div>
<div class="sect3">
<h4 id="_configuring_a_trigger_in_google_cloud"><a class="anchor" href="#_configuring_a_trigger_in_google_cloud"></a>Configuring a trigger in Google Cloud</h4>
<div class="paragraph">
<p>Once the Github repository is linked to the Google Cloud project, we can now set up a trigger that will
monitor the creation of new tags on Github. Each time a tag is created, this trigger will automatically
initiate the build process.</p>
</div>
<div class="paragraph">
<p>Navigate to the <code>Triggers</code> page in the Google Cloud Console.</p>
</div>
<div class="paragraph">
<p>Then click on the <code>CREATE TRIGGER</code> button.</p>
</div>
<div class="paragraph">
<p>Fill a name for the trigger, choose a region and in the <code>Event</code> section, choose <code>Push new tag</code>.</p>
</div>
<div class="paragraph">
<p>Then in the <code>Source</code> section choose <code>2nd gen</code>, select the Data Foundation repository we
previously linked and choose <code>.* (any tag)</code>.</p>
</div>
<div class="paragraph">
<p>Leave the default settings for the section <code>Configuration</code></p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The section <code>Configuration</code> values means that Cloud Build will use the <code>cloudbuild.yaml</code> file
at the root of the repository to execute the build process.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>In the <code>Advanced</code> section, add the <code>_GCS_BUCKET</code> substitution variable and assign it the
value of the bucket where the logs will be stored. In our case, it is <code>cortex-demo-logs-bucket</code>.</p>
</div>
<div class="paragraph">
<p>Then click on the <code>SAVE</code> button to create the trigger. We have successfully set up a trigger that will
automatically initiate the build process each time a new tag is created on Github.</p>
</div>
</div>
<div class="sect3">
<h4 id="_running_the_cicd_workflow"><a class="anchor" href="#_running_the_cicd_workflow"></a>Running the CI/CD workflow</h4>
<div class="sect4">
<h5 id="_update_the_configuration_file_config_json"><a class="anchor" href="#_update_the_configuration_file_config_json"></a>Update the configuration file <code>config.json</code></h5>
<div class="paragraph">
<p>First we need to update the configuration file <code>config.json</code>. Update it as we did it <a href="#config-json-id">here</a>.
Then commit the changes by running the following commands :</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">git add config/config.json
git commit -m "update config file"</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="_update_the_build_file_cloudbuild_yaml"><a class="anchor" href="#_update_the_build_file_cloudbuild_yaml"></a>Update the build file <code>cloudbuild.yaml</code></h5>
<div class="paragraph">
<p>Now we need to update the build file <code>cloudbuild.yaml</code> to include the new parameters.
Update the <code>cloudbuild.yaml</code> file with the following content :</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

steps:
  - name: gcr.io/cloud-builders/git <i class="conum" data-value="1"></i><b>(1)</b>
    entrypoint: "bash"
    id: 'init_git_env'
    waitFor: ['-']
    args:
      - "-c"
      - |-
        echo "Cleaning /workspace"
        rm -rf /workspace/*
        echo "Cloning the repository from Github"
        git clone --recurse-submodules "${_GIT_URL}" "${_GIT_PATH}"

  # init_deployment_config.py leaves the validated config.json file in workspace/config so it's available for other build steps
  - name: gcr.io/kittycorn-public/deploy-kittycorn:v2.0
    entrypoint: "bash"
    id: 'init_deploy_config'
    waitFor: ['init_git_env']
    args:
      - "-c"
      - |-
        set -e
        echo "Initial configuration ${_CONFIG_FILE}:"
        cat ${_CONFIG_FILE}

        python3 src/common/init_deployment_config.py --config-file "${_CONFIG_FILE}" \
              --sub-validator "src/SAP/SAP_REPORTING" \
              --sub-validator "src/SFDC/src" \
              --sub-validator "src/marketing/src"

        echo "Processed configuration:"
        cat ${_CONFIG_FILE}
        echo -e "\n--------------------------------"
    dir: ${_GIT_PATH} <i class="conum" data-value="2"></i><b>(2)</b>

  # remove the content of the DAGs bucket
  - name: gcr.io/cloud-builders/gsutil <i class="conum" data-value="3"></i><b>(3)</b>
    id: 'clear_airflow_dags_bucket'
    waitFor: ['init_deploy_config']
    entrypoint: "bash"
    args:
      - "-c"
      - |-
        echo "Removing existing DAGs in the Airflow bucket ${_AIRFLOW_BUCKET}"
        gsutil -m rm -r gs://${_AIRFLOW_BUCKET}/*

  - name: gcr.io/kittycorn-public/deploy-kittycorn:v2.0
    id: 'k9-pre'
    waitFor: ['clear_airflow_dags_bucket'] <i class="conum" data-value="4"></i><b>(4)</b>
    entrypoint: "bash"
    args:
      - "-c"
      - |-
        export PYTHONPATH=$$PYTHONPATH:src/:.
        python3 src/k9/src/deploy_k9.py \
                --config-file "${_CONFIG_FILE}" \
                --stage pre \
                --logs-bucket "${_GCS_BUCKET}"
    dir: ${_GIT_PATH} <i class="conum" data-value="2"></i><b>(2)</b>

  - name: gcr.io/kittycorn-public/deploy-kittycorn:v2.0
    entrypoint: "bash"
    id: 'sap-test-harness'
    waitFor: ['clear_airflow_dags_bucket'] <i class="conum" data-value="4"></i><b>(4)</b>
    args:
      - "-c"
      - |-
        _DEPLOY_SAP_=$(jq -r ."deploySAP" "${_CONFIG_FILE}")
        _TEST_DATA_=$(jq -r ."testData" "${_CONFIG_FILE}")
        _SQL_FLAVOUR_=$(jq -r ."SAP.SQLFlavor" "${_CONFIG_FILE}")
        _SQL_FLAVOUR_UP_=$(echo "$${_SQL_FLAVOUR_}" | tr '[:lower:]' '[:upper:]')
        if [[ "${_NO_TEST_DATA}" != "true" &amp;&amp; "$${_TEST_DATA_}" == "true" &amp;&amp; "$${_DEPLOY_SAP_}" == "true" &amp;&amp; "$${_SQL_FLAVOUR_}" != "union" ]]; then
          python3 src/common/create_test_harness.py --workload "SAP" --dataset "raw$${_SQL_FLAVOUR_UP_}"
        else
          echo "==Skipping Test Harness for SAP=="
        fi
    dir: ${_GIT_PATH} <i class="conum" data-value="2"></i><b>(2)</b>

  - name: gcr.io/kittycorn-public/deploy-kittycorn:v2.0
    entrypoint: "bash"
    id: 'sfdc-test-harness'
    waitFor: ['clear_airflow_dags_bucket'] <i class="conum" data-value="4"></i><b>(4)</b>
    args:
      - "-c"
      - |-
        _DEPLOY_SFDC_=$(jq -r ."deploySFDC" "${_CONFIG_FILE}")
        _TEST_DATA_=$(jq -r ."testData" "${_CONFIG_FILE}")
        if [[ "${_NO_TEST_DATA}" != "true" &amp;&amp; "$${_TEST_DATA_}" == "true" &amp;&amp; "$${_DEPLOY_SFDC_}" == "true" ]]; then
          python3 src/common/create_test_harness.py --workload "SFDC" --dataset "raw"
        else
          echo "==Skipping Test Harness for SFDC=="
        fi
    dir: ${_GIT_PATH} <i class="conum" data-value="2"></i><b>(2)</b>

  - name: gcr.io/kittycorn-public/deploy-kittycorn:v2.0
    id: 'sap-cdc'
    waitFor: ['sap-test-harness', 'k9-pre']
    entrypoint: "bash"
    args:
      - "-c"
      - |-
        _DEPLOY_SAP_=$(jq -r ."deploySAP" "${_CONFIG_FILE}")
        _DEPLOY_CDC_=$(jq -r ."SAP.deployCDC" "${_CONFIG_FILE}")
        _SQL_FLAVOUR_=$(jq -r ."SAP.SQLFlavor" "${_CONFIG_FILE}")

        if [[ "$${_DEPLOY_SAP_}" == "true" &amp;&amp; "$${_SQL_FLAVOUR_}" != "union"  &amp;&amp; "$${_DEPLOY_CDC_}" == "true" ]]; then
          cp -f "${_CONFIG_FILE}" src/SAP/SAP_CDC/config/config.json
          gcloud builds submit ./src/SAP/SAP_CDC \
            --config=./src/SAP/SAP_CDC/cloudbuild.cdc.yaml \
            --substitutions=_GCS_BUCKET='${_GCS_BUCKET}'
        else
          echo "==Skipping CDC for SAP=="
        fi
    dir: ${_GIT_PATH} <i class="conum" data-value="2"></i><b>(2)</b>

  - name: gcr.io/kittycorn-public/deploy-kittycorn:v2.0
    id: 'sap-reporting'
    waitFor: ['sap-cdc']
    entrypoint: "bash"
    args:
      - "-c"
      - |-
        _DEPLOY_SAP_=$(jq -r ."deploySAP" "${_CONFIG_FILE}")
        if [[ "$${_DEPLOY_SAP_}" == "true" ]]; then
          cp -f "${_CONFIG_FILE}" src/SAP/SAP_REPORTING/config/config.json
          gcloud builds submit ./src/SAP/SAP_REPORTING \
            --config=./src/SAP/SAP_REPORTING/cloudbuild.reporting.yaml \
            --substitutions=_GCS_BUCKET='${_GCS_BUCKET}'
        else
          echo "==Skipping Reporting for SAP=="
        fi
    dir: ${_GIT_PATH} <i class="conum" data-value="2"></i><b>(2)</b>

  - name: gcr.io/kittycorn-public/deploy-kittycorn:v2.0
    id: 'sap-ml-models'
    waitFor: ['sap-reporting']
    entrypoint: "bash"
    args:
      - "-c"
      - |-
        if [[ "${_DEPLOY_SAP_ML_MODELS}" == "true" ]]
        then
          _DEPLOY_SAP_=$(jq -r ."deploySAP" "${_CONFIG_FILE}")
          _SQL_FLAVOUR_=$(jq -r ."SAP.SQLFlavor" "${_CONFIG_FILE}")
          _PJID_SRC_=$(jq -r ."projectIdSource" "${_CONFIG_FILE}")
          _PJID_TGT_=$(jq -r ."projectIdTarget" "${_CONFIG_FILE}")
          _LOCATION_=$(jq -r ."location" "${_CONFIG_FILE}")
          _MANDT_=$(jq -r ."SAP.mandt" "${_CONFIG_FILE}")
          _DS_MODELS_=$(jq -r ."SAP.datasets.ml" "${_CONFIG_FILE}")
          _DS_REPORTING_=$(jq -r ."SAP.datasets.reporting" "${_CONFIG_FILE}")
          _DS_RAW_=$(jq -r ."SAP.datasets.raw" "${_CONFIG_FILE}")
          _DS_CDC_=$(jq -r ."SAP.datasets.cdc" "${_CONFIG_FILE}")

          if [[ "$${_DEPLOY_SAP_}" == "true" &amp;&amp; "$${_SQL_FLAVOUR_}" != "union" ]]; then
            gcloud builds submit ./src/SAP/SAP_ML_MODELS \
              --config=./src/SAP/SAP_ML_MODELS/cloudbuild.models.yaml \
              --substitutions=_PJID_SRC=$$_PJID_SRC_,_PJID_TGT=$$_PJID_TGT_,_DS_RAW=$$_DS_RAW_,_DS_CDC=$$_DS_CDC_,_DS_REPORTING=$$_DS_REPORTING_,_DS_MODELS=$$_DS_MODELS_,_SQL_FLAVOUR=$$_SQL_FLAVOUR_,_LOCATION=$$_LOCATION_,_MANDT=$$_MANDT_,_GCS_BUCKET='${_GCS_BUCKET}'
          else
            echo "==Skipping ML for SAP=="
          fi
        else
          echo "==Skipping ML for SAP=="
        fi
    dir: ${_GIT_PATH} <i class="conum" data-value="2"></i><b>(2)</b>

  - name: gcr.io/kittycorn-public/deploy-kittycorn:v2.0
    id: 'sfdc-deploy'
    waitFor: ['sfdc-test-harness', 'k9-pre']
    entrypoint: "bash"
    args:
      - "-c"
      - |-
        _DEPLOY_SFDC_=$(jq -r ."deploySFDC" "${_CONFIG_FILE}")
        if [[ "$${_DEPLOY_SFDC_}" == "true" ]]; then
          cp -f "${_CONFIG_FILE}" src/SFDC/config/config.json
          gcloud builds submit ./src/SFDC \
            --config=./src/SFDC/cloudbuild.sfdc.yaml \
            --substitutions=_GCS_BUCKET="${_GCS_BUCKET}"
        else
          echo "==Skipping SFDC=="
        fi
    dir: ${_GIT_PATH} <i class="conum" data-value="2"></i><b>(2)</b>

  - name: gcr.io/kittycorn-public/deploy-kittycorn:v2.0
    entrypoint: "bash"
    id: 'marketing-test-harness'
    waitFor: ['clear_airflow_dags_bucket'] <i class="conum" data-value="4"></i><b>(4)</b>
    args:
      - "-c"
      - |-
        _DEPLOY_MARKETING_=$(jq -r ."deployMarketing" "${_CONFIG_FILE}")
        _TEST_DATA_=$(jq -r ."testData" "${_CONFIG_FILE}")
        if [[ "${_NO_TEST_DATA}" != "true" &amp;&amp; "$${_TEST_DATA_}" == "true" &amp;&amp; "$${_DEPLOY_MARKETING_}" == "true" ]]; then
          _DEPLOY_ADS_=$(jq -r ."marketing.deployGoogleAds" "${_CONFIG_FILE}")
          _DEPLOY_CM360_=$(jq -r ."marketing.deployCM360" "${_CONFIG_FILE}")
          _DEPLOY_TIKTOK_=$(jq -r ."marketing.deployTikTok" "${_CONFIG_FILE}")
          _DEPLOY_LIVERAMP_=$(jq -r ."marketing.deployLiveRamp" "${_CONFIG_FILE}")
          _DEPLOY_META_=$(jq -r ."marketing.deployMeta" "${_CONFIG_FILE}")
          _DEPLOY_SFMC_=$(jq -r ."marketing.deploySFMC" "${_CONFIG_FILE}")
          if [[ "$${_DEPLOY_ADS_}" == "true" ]]; then
            echo "Deploying Google Ads Test Harness."
            python3 src/common/create_test_harness.py --workload "marketing.GoogleAds" --dataset "raw"
          fi
          if [[ "$${_DEPLOY_CM360_}" == "true" ]]; then
            echo "Deploying CM360 Test Harness."
            python3 src/common/create_test_harness.py --workload "marketing.CM360" --dataset "raw"
          fi
          if [[ "$${_DEPLOY_TIKTOK_}" == "true" ]]; then
            echo "Deploying TikTok Test Harness."
            python3 src/common/create_test_harness.py --workload "marketing.TikTok" --dataset "raw"
          fi
          if [[ "$${_DEPLOY_LIVERAMP_}" == "true" ]]; then
            echo "Deploying LiveRamp Test Harness."
            python3 src/common/create_test_harness.py --workload "marketing.LiveRamp" --dataset "cdc"
          fi
          if [[ "$${_DEPLOY_META_}" == "true" ]]; then
            echo "Deploying Meta Test Harness."
            python3 src/common/create_test_harness.py --workload "marketing.Meta" --dataset "raw"
          fi
          if [[ "$${_DEPLOY_SFMC_}" == "true" ]]; then
            echo "Deploying SFMC Test Harness."
            python3 src/common/create_test_harness.py --workload "marketing.SFMC" --dataset "raw"
          fi
        else
          echo "==Skipping Test Harness for Marketing=="
        fi
    dir: ${_GIT_PATH} <i class="conum" data-value="2"></i><b>(2)</b>

  - name: gcr.io/kittycorn-public/deploy-kittycorn:v2.0
    id: 'marketing-deploy'
    waitFor: ['k9-pre', 'marketing-test-harness']
    entrypoint: "bash"
    args:
      - "-c"
      - |-
        _DEPLOY_MARKETING_=$(jq -r ."deployMarketing" "${_CONFIG_FILE}")
        if [[ "$${_DEPLOY_MARKETING_}" == "true" ]]; then
          cp -f "${_CONFIG_FILE}" src/marketing/config/config.json
          gcloud builds submit ./src/marketing \
            --config=./src/marketing/cloudbuild.marketing.yaml \
            --substitutions=_GCS_LOGS_BUCKET="${_GCS_BUCKET}"
        else
          echo "==Skipping Marketing=="
        fi
    dir: ${_GIT_PATH} <i class="conum" data-value="2"></i><b>(2)</b>

  - name: gcr.io/kittycorn-public/deploy-kittycorn:v2.0
    id: 'k9-post'
    waitFor: ['k9-pre', 'sap-reporting', 'sfdc-deploy', 'marketing-deploy']
    entrypoint: "bash"
    args:
      - "-c"
      - |-
        export PYTHONPATH=$$PYTHONPATH:src/:.
        python3 src/k9/src/deploy_k9.py \
                --config-file "${_CONFIG_FILE}" \
                --stage post \
                --logs-bucket "${_GCS_BUCKET}"
    dir: ${_GIT_PATH} <i class="conum" data-value="2"></i><b>(2)</b>

  - name: gcr.io/kittycorn-public/credly:latest
    id: 'credly-badge'
    waitFor: ['k9-pre', 'sap-reporting', 'sfdc-deploy']
    entrypoint: "bash"
    args:
      - "-c"
      - |-
        _FLAG_=$(jq -r ."shareWithCredly" "${_CONFIG_FILE}")
        if [[ "$${_FLAG_}" != "true" ]]; then
          echo "===Skipping assigning a badge==="
        else
          _EMAIL_=$(jq -r ."userInfo.email" "${_CONFIG_FILE}")
          _FN_=$(jq -r ."userInfo.firstName" "${_CONFIG_FILE}")
          _LN_=$(jq -r ."userInfo.lastName" "${_CONFIG_FILE}")
          cd /usr/src/app
          if [ -z "$${_EMAIL_}" ] || [ -z "$${_FN_}" ] || [ -z "$${_LN_}" ] || ! [ -f credly.py ]; then
            echo "===Email, first name and last name are required in config.json==="
            echo "===Skipping assigning a badge==="
          else
            python3 credly.py --fn "$${_FN_}" --ln "$${_LN_}" --email "$${_EMAIL_}" || exit 0
          fi
        fi
    dir: ${_GIT_PATH} <i class="conum" data-value="2"></i><b>(2)</b>

  # copy generated dags to airflow bucket
  - name: gcr.io/cloud-builders/gsutil <i class="conum" data-value="5"></i><b>(5)</b>
    id: 'copy_dags_to_airflow'
    waitFor: ['credly-badge']
    entrypoint: "bash"
    args:
      - "-c"
      - |-
        _TARGET_BUCKET_=$(jq -r ."targetBucket" "${_CONFIG_FILE}")
        echo "Copying DAGs from ${_TARGET_BUCKET_} to ${_AIRFLOW_BUCKET}"
        gsutil -m cp -r gs://$${_TARGET_BUCKET_}/dags/ gs://${_AIRFLOW_BUCKET}/
        gsutil -m cp -r gs://$${_TARGET_BUCKET_}/data/ gs://${_AIRFLOW_BUCKET}/

logsBucket: "gs://$_GCS_BUCKET"
timeout: 32400s
substitutions:
  _CONFIG_FILE: "config/config.json"
  _DEPLOY_SAP_ML_MODELS: "false"
  _NO_TEST_DATA: "false"
  _GIT_URL: "https://github.com/DonnescoPablo/cortex-data-foundation.git" <i class="conum" data-value="6"></i><b>(6)</b>
  _GIT_PATH: "cortex-data-foundation" <i class="conum" data-value="7"></i><b>(7)</b>
  _AIRFLOW_BUCKET: "southamerica-east1-cortex-d-3a0e66a5-bucket" <i class="conum" data-value="8"></i><b>(8)</b>
options:
  substitution_option: "ALLOW_LOOSE"
tags: ["cortex"]</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Clone the repository from Github.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Set working directory for build steps.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Remove the content of the DAGs bucket.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Wait for the <code>clear_airflow_dags_bucket</code> to complete before executing the next steps.</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Copy the generated DAGs to the Airflow bucket.</td>
</tr>
<tr>
<td><i class="conum" data-value="6"></i><b>6</b></td>
<td>The URL of the forked Cortex Data Foundation repository.</td>
</tr>
<tr>
<td><i class="conum" data-value="7"></i><b>7</b></td>
<td>The name of the directory where the repository will be cloned.</td>
</tr>
<tr>
<td><i class="conum" data-value="8"></i><b>8</b></td>
<td>Airflow bucket where the DAGs will be stored.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Then commit the changes by running the following commands :</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">git add cloudbuild.yaml
git commit -m "update build file"</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="_update_the_reporting_settings_file"><a class="anchor" href="#_update_the_reporting_settings_file"></a>Update the reporting settings file</h5>
<div class="paragraph">
<p>Now let&#8217;s update the SAP reporting settings file <code>src/SAP/SAP_REPORTING/reporting_settings_ecc.yaml</code> to include the new parameters.
In the file, update the refresh frequency of the tables <code>SalesOrders</code> and <code>SalesOrders_V2</code> to 5 minutes.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">// ... <i class="conum" data-value="1"></i><b>(1)</b>

bq_independent_objects:

// ... <i class="conum" data-value="1"></i><b>(1)</b>

    - sql_file: SalesOrders_V2.sql
        description: "Sales Orders Header and Items"
        type: table
        table_setting:
            load_frequency: "*/5 * * * *" <i class="conum" data-value="2"></i><b>(2)</b>

// ... <i class="conum" data-value="1"></i><b>(1)</b>

    - sql_file: SalesOrders.sql
        description: "Sales Orders Header and Items"
        type: table
        table_setting:
            load_frequency: "*/5 * * * *" <i class="conum" data-value="2"></i><b>(2)</b>

// ... <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>There are other configurations in the file that are not shown here.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The new refresh frequency of the tables.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Then commit and push the changes by running the following commands :</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">cd src/SAP/SAP_REPORTING
git add reporting_settings_ecc.yaml
git commit -m "update SAP reporting settings file"
git push origin HEAD:main
cd ../../../
git push</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="_trigger_the_build_process"><a class="anchor" href="#_trigger_the_build_process"></a>Trigger the build process</h5>
<div class="paragraph">
<p>Now that the changes are committed and pushed to the repository, we can create a new tag to trigger the build process.</p>
</div>
<div class="paragraph">
<p>Navigate to the forked Cortex Data Foundation repository on Github and click on <code>Tags</code>.</p>
</div>
<div class="paragraph">
<p>Then click on the <code>Create a new release</code> button.</p>
</div>
<div class="paragraph">
<p>Click on the <code>Choose a tag</code>, enter a tag name <code>v1.0.0</code> and click on <code>Create new tag: v1.0.0 on publish</code>.</p>
</div>
<div class="paragraph">
<p>Then add a release title and a description and click on the <code>Publish release</code> button.</p>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_using_looker_to_visualize_the_data"><a class="anchor" href="#_using_looker_to_visualize_the_data"></a>Using Looker to visualize the data</h3>
<div class="paragraph">
<p>We can use Looker to visualize the data stored in BigQuery. There are predefined dashboards that can be used to visualize the data.
In this section, we are going to set up predefined SAP dashboards in Looker.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>A Looker instance is needed to visualize the data. In this guide, we are going to use the Looker instance
provided by Capgemini. To get access to the Looker instance, contact a Looker Capgemini administrator.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="_connecting_looker_to_bigquery"><a class="anchor" href="#_connecting_looker_to_bigquery"></a>Connecting Looker to BigQuery</h4>
<div class="paragraph">
<p>Before setting up predefined SAP dashboards in Looker, we need to set up a connection between Looker and BigQuery.
One way that Looker can authenticate into your BigQuery database is with a BigQuery service account.</p>
</div>
<div id="create-service-account-id" class="paragraph">
<p>Follow this <a href="https://cloud.google.com/looker/docs/db-config-google-bigquery#creating_a_service_account_and_downloading_the_json_credentials_certificate">tutorial</a> to create a BigQuery service account and download the JSON credentials certificate.</p>
</div>
<div class="paragraph">
<p>Once the service account is created, navigate to the Looker instance and set up the connection to BigQuery
by following this <a href="https://cloud.google.com/looker/docs/db-config-google-bigquery#connecting-to">guide</a>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>During the connection setup in Looker, fill the <code>Dataset</code> field with <code>SAP_REPORTING</code>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Now that the connection is set up, we can set up predefined SAP dashboards in Looker.</p>
</div>
</div>
<div class="sect3">
<h4 id="_setting_up_predefined_sap_dashboards_in_looker"><a class="anchor" href="#_setting_up_predefined_sap_dashboards_in_looker"></a>Setting up predefined SAP dashboards in Looker</h4>
<div class="paragraph">
<p>First we have to fork the <a href="https://github.com/looker-open-source/block-cortex-sap">block-cortex-sap</a> repository.</p>
</div>
<div class="paragraph">
<p>Then in the <a href="https://capgeminifrance.cloud.looker.com">Looker instance&#8217;s</a> homepage,
access the Looker Marketplace by clicking on <code>Manage</code> as shown in the image below.</p>
</div>
<div class="paragraph">
<p>In the Marketplace&#8217;s page, click on <code>Install via Git URL</code> as shown in the image below.</p>
</div>
<div class="paragraph">
<p>In the opened window, fill in the <code>Git Repository URL</code> field with the forked repository URL,
the <code>Git Commit SHA</code> field with the commit SHA of the forked repository and click on the <code>Install</code> button.</p>
</div>
<div class="paragraph">
<p>Then click on <code>Agree and Continue</code></p>
</div>
<div class="paragraph">
<p>In the opened window, fill in the <code>Connection Name</code> field with the BigQuery connection we previously set up,
the <code>GCP Project Name</code> field with the project ID, the other fields as shown in the image below and click on the <code>Install</code> button.</p>
</div>
<div class="paragraph">
<p>We can see that the installation is successful.</p>
</div>
</div>
<div class="sect3">
<h4 id="_accessing_the_predefined_sap_dashboards_in_looker"><a class="anchor" href="#_accessing_the_predefined_sap_dashboards_in_looker"></a>Accessing the predefined SAP dashboards in Looker</h4>
<div class="paragraph">
<p>Now we can access the predefined SAP dashboards in Looker. Click on the <code>Main menu</code> button. In the slide menu,
click on <code>Blocks</code> and among the appeared blocks, click on <code>Cortex Data Foundation for SAP &#8230;&#8203;</code> as shown below.</p>
</div>
<div class="paragraph">
<p>Among the dashboards that appear, click on <code>SAP Sales Orders</code> to visualize the data.</p>
</div>
<div class="paragraph">
<p>We can see that the data is visualized in the dashboard. Notice that we have 6837 Total Orders.</p>
</div>
<div class="paragraph">
<p>We can even see the SQL query associated with the report. To do so, click on the <code>Explore from here</code> button.</p>
</div>
<div class="paragraph">
<p>Then click on the <code>SQL</code> tab.</p>
</div>
<div class="paragraph">
<p>In this section, we have set up predefined SAP dashboards in Looker to visualize the data stored in BigQuery.
We can also create custom dashboards in Looker to visualize the data. However, Looker is not the only tool
that can be used to visualize the data. We can also use PowerBI to visualize the data. The next section will show
how to use PowerBI to visualize the data.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>In opposition to Looker, there are not predefined dashboards in PowerBI. We have to ower own dashboards.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_using_powerbi_to_visualize_the_data"><a class="anchor" href="#_using_powerbi_to_visualize_the_data"></a>Using PowerBI to visualize the data</h3>
<div class="paragraph">
<p>We can use PowerBI to visualize the data stored in BigQuery. In this section, we are going to set up a connection
between PowerBI and BigQuery and create a custom dashboard to visualize the data.</p>
</div>
<div class="paragraph">
<p>First, like in the previous section, we need to create a service account to use for the connection to PowerBI.
Follow the <a href="#create-service-account-id">same steps</a> to create a service account and download the JSON credentials certificate.
For the permissions, grant the <code>Editor</code> role to the service account.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p>It is always recommended to have least privileges on resources, so we will need to adjust the permissions later.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Now that the service account is created and the JSON credentials certificate is downloaded, we are going to
connect to BigQuery from PowerBI.</p>
</div>
<div class="paragraph">
<p>In the PowerBI Desktop, click on <code>Rapport Vierge</code>.</p>
</div>
<div class="paragraph">
<p>Then Click on <code>Obtenir les donnÃ©es</code> and <code>Plus&#8230;&#8203;</code>.</p>
</div>
<div class="paragraph">
<p>In the opened window search for <code>BigQuery</code> and click on <code>Se connecter</code>.</p>
</div>
<div class="paragraph">
<p>Then fill in the information as shown in the image below and click on <code>OK</code>.</p>
</div>
<div class="paragraph">
<p>Click on <code>Connexion au compte de service</code> and enter the service account email
and the content of the JSON credentials certificate and click on <code>Se connecter</code>.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p>You have to remove the line breaks in the JSON credentials certificate before pasting it.
Else, the connection will fail.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Once the connection is successful,select the <code>SalesOrders</code> table and click on <code>Charger</code>.</p>
</div>
<div class="paragraph">
<p>Choose <code>DirectQuery</code> as the data connectivity mode and click on <code>OK</code>.</p>
</div>
<div class="paragraph">
<p>Once the table is loaded, we can create a custom dashboard to visualize the data.</p>
</div>
<div class="paragraph">
<p>We are going to create a simple report that shows the total number of orders as in the Looker dashboard.
To do so, click on <code>Nouvelle mesure</code> and fill in the instruction below :</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">Sales Orders Count Orders =
CALCULATE(
    COUNT(SalesOrders_V2[SalesDocument_VBELN]),
    SalesOrders_V2[DocumentCategory_VBTYP] = "C",
    SalesOrders_V2[CreationDate_ERDAT] &gt;= DATE(2022, 1, 1),
    SalesOrders_V2[CreationDate_ERDAT] &lt; DATE(2022, 4, 22),
    SalesOrders_V2[Client_MANDT] = "100"
)</code></pre>
</div>
</div>
<div class="paragraph">
<p>And validate it as shown in the image below.</p>
</div>
<div class="paragraph">
<p>Then in the <code>DonnÃ©es</code> panel, drag and drop the <code>Sales Orders Count Orders</code> measure to the
report area.</p>
</div>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <p>This page was built using the Antora default UI.</p>
  <p>The source code for this UI is licensed under the terms of the MPL-2.0 license.</p>
</footer>
<script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
